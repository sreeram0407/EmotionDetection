{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T22:24:48.469806Z",
     "start_time": "2024-12-03T22:24:45.381506Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import cv2\n",
    "import os\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T22:25:22.255579Z",
     "start_time": "2024-12-03T22:25:22.250314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_fer2013():\n",
    "    \"\"\"Load and preprocess FER2013 dataset.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('fer2013.csv', names=['emotion', 'pixels', 'Usage'], skiprows=1)\n",
    "        print(\"Dataset loaded successfully!\")\n",
    "        print(f\"Total samples: {len(df)}\")\n",
    "        pixels = df['pixels'].str.split(' ').apply(lambda x: [int(pixel) for pixel in x])\n",
    "        X = np.array(pixels.tolist())\n",
    "        X = X.reshape(-1, 48, 48, 1).astype('float32')\n",
    "        X = X / 255.0\n",
    "        y = df['emotion'].values\n",
    "        \n",
    "        print(f\"Input shape: {X.shape}\")\n",
    "        print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "        print(\"\\nClass distribution:\")\n",
    "        for emotion_id, count in enumerate(np.bincount(y)):\n",
    "            print(f\"Emotion {emotion_id}: {count} samples\")     \n",
    "        return X, y, df['Usage'].values\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {str(e)}\")\n",
    "        return None, None, None"
   ],
   "id": "a93d5c7af00e4a69",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T22:25:43.625022Z",
     "start_time": "2024-12-03T22:25:43.619141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_model():\n",
    "    \"\"\"Create CNN model for facial emotion recognition.\"\"\"\n",
    "    model = Sequential([\n",
    "        # First Convolution Block\n",
    "        Conv2D(96, (3, 3), padding='same', activation='relu', input_shape=(48, 48, 1)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(96, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second Convolution Block\n",
    "        Conv2D(192, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(192, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third Convolution Block\n",
    "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(384, (3, 3), padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Dense Layers\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(7, activation='softmax')\n",
    "    ]) \n",
    "    return model"
   ],
   "id": "9edbcfc1543c7cd9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T22:26:01.538814Z",
     "start_time": "2024-12-03T22:26:01.530754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model(continue_training=False):\n",
    "    \"\"\"Train the emotion detection model.\"\"\"\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading dataset...\")\n",
    "    X, y, usage = load_fer2013()\n",
    "    \n",
    "    if X is None:\n",
    "        return None, None\n",
    "    \n",
    "    # Split data\n",
    "    X_train = X[usage == 'Training']\n",
    "    y_train = y[usage == 'Training']\n",
    "    X_test = X[usage == 'PrivateTest']\n",
    "    y_test = y[usage == 'PrivateTest']\n",
    "    \n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Test samples: {len(X_test)}\")\n",
    "    \n",
    "    # Data augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    if continue_training and os.path.exists('checkpoints/best_model.h5'):\n",
    "        print(\"\\nLoading existing model for continued training...\")\n",
    "        model = load_model('checkpoints/best_model.h5')\n",
    "    else:\n",
    "        print(\"\\nCreating new model...\")\n",
    "        model = create_model()\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # Create checkpoint directory\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(checkpoint_dir, 'best_model.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            patience=15,\n",
    "            verbose=1,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=8,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining model...\")\n",
    "    history = model.fit(\n",
    "        datagen.flow(X_train, y_train, batch_size=32),\n",
    "        validation_data=(X_test, y_test),\n",
    "        steps_per_epoch=len(X_train) // 32,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    return model, history"
   ],
   "id": "2f484f555ef7eab2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T22:26:19.406033Z",
     "start_time": "2024-12-03T22:26:19.393985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_video(input_path, output_path):\n",
    "    \"\"\"Process a video file with emotion detection.\"\"\"\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model('checkpoints/best_model.h5')\n",
    "    \n",
    "    # Load face cascade\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Emotion labels\n",
    "    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    # Open video\n",
    "    print(\"Opening video file...\")\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=4,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        # Process each face\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "            face_roi = cv2.resize(face_roi, (48, 48))\n",
    "            face_roi = face_roi.astype('float32') / 255.0\n",
    "            face_roi = np.expand_dims(face_roi, axis=[0, -1])\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model.predict(face_roi, verbose=0)[0]\n",
    "            \n",
    "            # Get top 2 emotions\n",
    "            top_2_idx = np.argsort(predictions)[-2:][::-1]\n",
    "            emotion_1 = emotions[top_2_idx[0]]\n",
    "            emotion_2 = emotions[top_2_idx[1]]\n",
    "            conf_1 = predictions[top_2_idx[0]]\n",
    "            conf_2 = predictions[top_2_idx[1]]\n",
    "            \n",
    "            # Draw rectangle and labels\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # First emotion\n",
    "            label_1 = f\"{emotion_1}: {conf_1:.2f}\"\n",
    "            label_size, _ = cv2.getTextSize(label_1, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n",
    "            cv2.rectangle(frame, \n",
    "                        (x, y - label_size[1] - 10), \n",
    "                        (x + label_size[0], y), \n",
    "                        (0, 255, 0), \n",
    "                        cv2.FILLED)\n",
    "            cv2.putText(frame, label_1, (x, y - 5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "            \n",
    "            # Second emotion\n",
    "            label_2 = f\"{emotion_2}: {conf_2:.2f}\"\n",
    "            label_size, _ = cv2.getTextSize(label_2, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(frame, \n",
    "                        (x, y - label_size[1] - 30), \n",
    "                        (x + label_size[0], y - 20), \n",
    "                        (255, 255, 0), \n",
    "                        cv2.FILLED)\n",
    "            cv2.putText(frame, label_2, (x, y - 25), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "        \n",
    "        # Display and write frame\n",
    "        cv2.imshow('Emotion Detection', frame)\n",
    "        out.write(frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"\\nProcessed video saved as: {output_path}\")"
   ],
   "id": "6386743e459d2656",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T22:27:11.626436Z",
     "start_time": "2024-12-03T22:27:11.618027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def start_webcam_detection():\n",
    "    \"\"\"Start real-time emotion detection using webcam.\"\"\"\n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = load_model('checkpoints/best_model.h5')\n",
    "    \n",
    "    # Load face cascade\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Emotion labels\n",
    "    emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    \n",
    "    # Start webcam\n",
    "    print(\"Starting webcam...\")\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Set webcam properties\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "    \n",
    "    # FPS calculation variables\n",
    "    fps_start_time = time.time()\n",
    "    fps = 0\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Can't receive frame from webcam\")\n",
    "            break\n",
    "            \n",
    "        # Calculate FPS\n",
    "        frame_count += 1\n",
    "        if frame_count >= 30:\n",
    "            fps = frame_count / (time.time() - fps_start_time)\n",
    "            fps_start_time = time.time()\n",
    "            frame_count = 0\n",
    "        \n",
    "        # Process frame\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=4,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        # Process each face\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "            face_roi = cv2.resize(face_roi, (48, 48))\n",
    "            face_roi = face_roi.astype('float32') / 255.0\n",
    "            face_roi = np.expand_dims(face_roi, axis=[0, -1])\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model.predict(face_roi, verbose=0)[0]\n",
    "            top_2_idx = np.argsort(predictions)[-2:][::-1]\n",
    "            \n",
    "            # Get emotions\n",
    "            emotion_1 = emotions[top_2_idx[0]]\n",
    "            emotion_2 = emotions[top_2_idx[1]]\n",
    "            conf_1 = predictions[top_2_idx[0]]\n",
    "            conf_2 = predictions[top_2_idx[1]]\n",
    "            \n",
    "            # Draw rectangle and labels\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # First emotion\n",
    "            label_1 = f\"{emotion_1}: {conf_1:.2f}\"\n",
    "            label_size, _ = cv2.getTextSize(label_1, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n",
    "            cv2.rectangle(frame, \n",
    "                        (x, y - label_size[1] - 10), \n",
    "                        (x + label_size[0], y), \n",
    "                        (0, 255, 0), \n",
    "                        cv2.FILLED)\n",
    "            cv2.putText(frame, label_1, (x, y - 5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 2)\n",
    "            \n",
    "            # Second emotion\n",
    "            label_2 = f\"{emotion_2}: {conf_2:.2f}\"\n",
    "            label_size, _ = cv2.getTextSize(label_2, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
    "            cv2.rectangle(frame, \n",
    "                        (x, y - label_size[1] - 30), \n",
    "                        (x + label_size[0], y - 20), \n",
    "                        (255, 255, 0), \n",
    "                        cv2.FILLED)\n",
    "            cv2.putText(frame, label_2, (x, y - 25), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
    "        \n",
    "        # Add FPS counter\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        \n",
    "        # Show frame\n",
    "        cv2.imshow('Emotion Detection (Press Q to quit)', frame)\n",
    "        \n",
    "        # Break loop on 'q' press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Clean up\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Webcam closed\")"
   ],
   "id": "d384712beb9f8c2c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:34:32.843327Z",
     "start_time": "2024-12-03T22:27:44.523405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set memory growth for GPU if available\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    if physical_devices:\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "    \n",
    "    # Train the model\n",
    "    continue_training = True \n",
    "    model, history = train_model(continue_training)\n",
    "    \n",
    "    if model is not None:\n",
    "        model.save('final_model.h5')\n",
    "        print(\"\\nTraining completed!\")\n",
    "        print(\"Models saved in 'checkpoints/best_model.h5' and 'final_model.h5'\")"
   ],
   "id": "28f6e137d431598b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully!\n",
      "Total samples: 26510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (26510, 48, 48, 1)\n",
      "Number of classes: 7\n",
      "\n",
      "Class distribution:\n",
      "Emotion 0: 3640 samples\n",
      "Emotion 1: 400 samples\n",
      "Emotion 2: 3763 samples\n",
      "Emotion 3: 6604 samples\n",
      "Emotion 4: 4515 samples\n",
      "Emotion 5: 2994 samples\n",
      "Emotion 6: 4594 samples\n",
      "Training samples: 19332\n",
      "Test samples: 3589\n",
      "\n",
      "Loading existing model for continued training...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 48, 48, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 24, 24, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 24, 24, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 12, 12, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 6, 6, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 9216)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               2359552   \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 1799      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2733831 (10.43 MB)\n",
      "Trainable params: 2732423 (10.42 MB)\n",
      "Non-trainable params: 1408 (5.50 KB)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training model...\n",
      "Epoch 1/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1558 - accuracy: 0.5622\n",
      "Epoch 1: val_accuracy improved from -inf to 0.61939, saving model to checkpoints/best_model.h5\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1558 - accuracy: 0.5622 - val_loss: 1.0103 - val_accuracy: 0.6194 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "  2/604 [..............................] - ETA: 34s - loss: 0.9900 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sreeram/Desktop/skonDatabase/HON/Contracts/EmotionDetection/.venv/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604/604 [==============================] - ETA: 0s - loss: 1.1601 - accuracy: 0.5622\n",
      "Epoch 2: val_accuracy did not improve from 0.61939\n",
      "604/604 [==============================] - 36s 59ms/step - loss: 1.1601 - accuracy: 0.5622 - val_loss: 1.1157 - val_accuracy: 0.5829 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1674 - accuracy: 0.5561\n",
      "Epoch 3: val_accuracy did not improve from 0.61939\n",
      "604/604 [==============================] - 35s 59ms/step - loss: 1.1674 - accuracy: 0.5561 - val_loss: 1.1207 - val_accuracy: 0.5773 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1711 - accuracy: 0.5599\n",
      "Epoch 4: val_accuracy did not improve from 0.61939\n",
      "604/604 [==============================] - 36s 59ms/step - loss: 1.1711 - accuracy: 0.5599 - val_loss: 1.0691 - val_accuracy: 0.5963 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1750 - accuracy: 0.5553\n",
      "Epoch 5: val_accuracy did not improve from 0.61939\n",
      "604/604 [==============================] - 36s 60ms/step - loss: 1.1750 - accuracy: 0.5553 - val_loss: 1.0344 - val_accuracy: 0.6074 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1683 - accuracy: 0.5548\n",
      "Epoch 6: val_accuracy did not improve from 0.61939\n",
      "604/604 [==============================] - 37s 61ms/step - loss: 1.1683 - accuracy: 0.5548 - val_loss: 1.1021 - val_accuracy: 0.5868 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1678 - accuracy: 0.5646\n",
      "Epoch 7: val_accuracy did not improve from 0.61939\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1678 - accuracy: 0.5646 - val_loss: 1.0346 - val_accuracy: 0.6177 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1665 - accuracy: 0.5584\n",
      "Epoch 8: val_accuracy did not improve from 0.61939\n",
      "604/604 [==============================] - 40s 67ms/step - loss: 1.1665 - accuracy: 0.5584 - val_loss: 1.0563 - val_accuracy: 0.5963 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.2062 - accuracy: 0.5412\n",
      "Epoch 9: val_accuracy did not improve from 0.61939\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 1.2062 - accuracy: 0.5412 - val_loss: 1.0194 - val_accuracy: 0.6160 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1729 - accuracy: 0.5569\n",
      "Epoch 10: val_accuracy improved from 0.61939 to 0.62023, saving model to checkpoints/best_model.h5\n",
      "604/604 [==============================] - 36s 59ms/step - loss: 1.1729 - accuracy: 0.5569 - val_loss: 1.0124 - val_accuracy: 0.6202 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1595 - accuracy: 0.5640\n",
      "Epoch 11: val_accuracy improved from 0.62023 to 0.62775, saving model to checkpoints/best_model.h5\n",
      "604/604 [==============================] - 37s 61ms/step - loss: 1.1595 - accuracy: 0.5640 - val_loss: 1.0003 - val_accuracy: 0.6278 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1522 - accuracy: 0.5617\n",
      "Epoch 12: val_accuracy did not improve from 0.62775\n",
      "604/604 [==============================] - 36s 59ms/step - loss: 1.1522 - accuracy: 0.5617 - val_loss: 1.0599 - val_accuracy: 0.6060 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1623 - accuracy: 0.5628\n",
      "Epoch 13: val_accuracy did not improve from 0.62775\n",
      "604/604 [==============================] - 35s 58ms/step - loss: 1.1623 - accuracy: 0.5628 - val_loss: 1.0027 - val_accuracy: 0.6269 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1520 - accuracy: 0.5667\n",
      "Epoch 14: val_accuracy did not improve from 0.62775\n",
      "604/604 [==============================] - 35s 58ms/step - loss: 1.1520 - accuracy: 0.5667 - val_loss: 0.9926 - val_accuracy: 0.6266 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1497 - accuracy: 0.5676\n",
      "Epoch 15: val_accuracy did not improve from 0.62775\n",
      "604/604 [==============================] - 36s 60ms/step - loss: 1.1497 - accuracy: 0.5676 - val_loss: 0.9951 - val_accuracy: 0.6255 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1521 - accuracy: 0.5670\n",
      "Epoch 16: val_accuracy did not improve from 0.62775\n",
      "604/604 [==============================] - 37s 62ms/step - loss: 1.1521 - accuracy: 0.5670 - val_loss: 1.0270 - val_accuracy: 0.6163 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1470 - accuracy: 0.5694\n",
      "Epoch 17: val_accuracy did not improve from 0.62775\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1470 - accuracy: 0.5694 - val_loss: 0.9950 - val_accuracy: 0.6252 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1443 - accuracy: 0.5650\n",
      "Epoch 18: val_accuracy improved from 0.62775 to 0.63054, saving model to checkpoints/best_model.h5\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1443 - accuracy: 0.5650 - val_loss: 0.9879 - val_accuracy: 0.6305 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1374 - accuracy: 0.5706\n",
      "Epoch 19: val_accuracy did not improve from 0.63054\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1374 - accuracy: 0.5706 - val_loss: 1.0047 - val_accuracy: 0.6244 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1363 - accuracy: 0.5724\n",
      "Epoch 20: val_accuracy did not improve from 0.63054\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 1.1363 - accuracy: 0.5724 - val_loss: 0.9889 - val_accuracy: 0.6272 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1382 - accuracy: 0.5723\n",
      "Epoch 21: val_accuracy did not improve from 0.63054\n",
      "604/604 [==============================] - 40s 67ms/step - loss: 1.1382 - accuracy: 0.5723 - val_loss: 1.0068 - val_accuracy: 0.6239 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1313 - accuracy: 0.5767\n",
      "Epoch 22: val_accuracy did not improve from 0.63054\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1313 - accuracy: 0.5767 - val_loss: 1.0011 - val_accuracy: 0.6252 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1349 - accuracy: 0.5723\n",
      "Epoch 23: val_accuracy did not improve from 0.63054\n",
      "604/604 [==============================] - 37s 62ms/step - loss: 1.1349 - accuracy: 0.5723 - val_loss: 1.0122 - val_accuracy: 0.6213 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1254 - accuracy: 0.5735\n",
      "Epoch 24: val_accuracy did not improve from 0.63054\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1254 - accuracy: 0.5735 - val_loss: 0.9978 - val_accuracy: 0.6241 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1211 - accuracy: 0.5773\n",
      "Epoch 25: val_accuracy did not improve from 0.63054\n",
      "604/604 [==============================] - 40s 66ms/step - loss: 1.1211 - accuracy: 0.5773 - val_loss: 0.9909 - val_accuracy: 0.6258 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1236 - accuracy: 0.5731\n",
      "Epoch 26: val_accuracy did not improve from 0.63054\n",
      "\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 1.1236 - accuracy: 0.5731 - val_loss: 0.9914 - val_accuracy: 0.6230 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1250 - accuracy: 0.5753\n",
      "Epoch 27: val_accuracy improved from 0.63054 to 0.63249, saving model to checkpoints/best_model.h5\n",
      "604/604 [==============================] - 39s 65ms/step - loss: 1.1250 - accuracy: 0.5753 - val_loss: 0.9891 - val_accuracy: 0.6325 - lr: 2.0000e-05\n",
      "Epoch 28/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1171 - accuracy: 0.5788\n",
      "Epoch 28: val_accuracy did not improve from 0.63249\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1171 - accuracy: 0.5788 - val_loss: 0.9857 - val_accuracy: 0.6314 - lr: 2.0000e-05\n",
      "Epoch 29/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1280 - accuracy: 0.5744\n",
      "Epoch 29: val_accuracy did not improve from 0.63249\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 1.1280 - accuracy: 0.5744 - val_loss: 0.9842 - val_accuracy: 0.6280 - lr: 2.0000e-05\n",
      "Epoch 30/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1173 - accuracy: 0.5776\n",
      "Epoch 30: val_accuracy did not improve from 0.63249\n",
      "604/604 [==============================] - 40s 66ms/step - loss: 1.1173 - accuracy: 0.5776 - val_loss: 0.9884 - val_accuracy: 0.6289 - lr: 2.0000e-05\n",
      "Epoch 31/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1247 - accuracy: 0.5772\n",
      "Epoch 31: val_accuracy did not improve from 0.63249\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1247 - accuracy: 0.5772 - val_loss: 0.9937 - val_accuracy: 0.6314 - lr: 2.0000e-05\n",
      "Epoch 32/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1237 - accuracy: 0.5782\n",
      "Epoch 32: val_accuracy did not improve from 0.63249\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1237 - accuracy: 0.5782 - val_loss: 0.9870 - val_accuracy: 0.6311 - lr: 2.0000e-05\n",
      "Epoch 33/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1169 - accuracy: 0.5798\n",
      "Epoch 33: val_accuracy did not improve from 0.63249\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1169 - accuracy: 0.5798 - val_loss: 0.9841 - val_accuracy: 0.6266 - lr: 2.0000e-05\n",
      "Epoch 34/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1216 - accuracy: 0.5745\n",
      "Epoch 34: val_accuracy did not improve from 0.63249\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1216 - accuracy: 0.5745 - val_loss: 0.9876 - val_accuracy: 0.6308 - lr: 2.0000e-05\n",
      "Epoch 35/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1296 - accuracy: 0.5741\n",
      "Epoch 35: val_accuracy improved from 0.63249 to 0.63416, saving model to checkpoints/best_model.h5\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1296 - accuracy: 0.5741 - val_loss: 0.9844 - val_accuracy: 0.6342 - lr: 2.0000e-05\n",
      "Epoch 36/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1241 - accuracy: 0.5752\n",
      "Epoch 36: val_accuracy did not improve from 0.63416\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1241 - accuracy: 0.5752 - val_loss: 0.9868 - val_accuracy: 0.6330 - lr: 2.0000e-05\n",
      "Epoch 37/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1209 - accuracy: 0.5770\n",
      "Epoch 37: val_accuracy did not improve from 0.63416\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1209 - accuracy: 0.5770 - val_loss: 0.9830 - val_accuracy: 0.6303 - lr: 2.0000e-05\n",
      "Epoch 38/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1182 - accuracy: 0.5766\n",
      "Epoch 38: val_accuracy did not improve from 0.63416\n",
      "604/604 [==============================] - 39s 65ms/step - loss: 1.1182 - accuracy: 0.5766 - val_loss: 0.9860 - val_accuracy: 0.6286 - lr: 2.0000e-05\n",
      "Epoch 39/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1229 - accuracy: 0.5774\n",
      "Epoch 39: val_accuracy did not improve from 0.63416\n",
      "604/604 [==============================] - 63s 105ms/step - loss: 1.1229 - accuracy: 0.5774 - val_loss: 0.9890 - val_accuracy: 0.6300 - lr: 2.0000e-05\n",
      "Epoch 40/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1188 - accuracy: 0.5760\n",
      "Epoch 40: val_accuracy did not improve from 0.63416\n",
      "604/604 [==============================] - 121s 201ms/step - loss: 1.1188 - accuracy: 0.5760 - val_loss: 0.9898 - val_accuracy: 0.6319 - lr: 2.0000e-05\n",
      "Epoch 41/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1160 - accuracy: 0.5781\n",
      "Epoch 41: val_accuracy improved from 0.63416 to 0.63555, saving model to checkpoints/best_model.h5\n",
      "604/604 [==============================] - 78s 129ms/step - loss: 1.1160 - accuracy: 0.5781 - val_loss: 0.9860 - val_accuracy: 0.6356 - lr: 2.0000e-05\n",
      "Epoch 42/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1188 - accuracy: 0.5790\n",
      "Epoch 42: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 160s 266ms/step - loss: 1.1188 - accuracy: 0.5790 - val_loss: 0.9827 - val_accuracy: 0.6305 - lr: 2.0000e-05\n",
      "Epoch 43/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1254 - accuracy: 0.5789\n",
      "Epoch 43: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 73s 120ms/step - loss: 1.1254 - accuracy: 0.5789 - val_loss: 0.9830 - val_accuracy: 0.6286 - lr: 2.0000e-05\n",
      "Epoch 44/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1164 - accuracy: 0.5772\n",
      "Epoch 44: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 231s 383ms/step - loss: 1.1164 - accuracy: 0.5772 - val_loss: 0.9840 - val_accuracy: 0.6336 - lr: 2.0000e-05\n",
      "Epoch 45/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1171 - accuracy: 0.5756\n",
      "Epoch 45: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 925s 2s/step - loss: 1.1171 - accuracy: 0.5756 - val_loss: 0.9837 - val_accuracy: 0.6297 - lr: 2.0000e-05\n",
      "Epoch 46/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1162 - accuracy: 0.5762\n",
      "Epoch 46: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1162 - accuracy: 0.5762 - val_loss: 0.9929 - val_accuracy: 0.6303 - lr: 2.0000e-05\n",
      "Epoch 47/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1198 - accuracy: 0.5804\n",
      "Epoch 47: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1198 - accuracy: 0.5804 - val_loss: 0.9823 - val_accuracy: 0.6311 - lr: 2.0000e-05\n",
      "Epoch 48/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1109 - accuracy: 0.5816\n",
      "Epoch 48: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 39s 65ms/step - loss: 1.1109 - accuracy: 0.5816 - val_loss: 0.9801 - val_accuracy: 0.6336 - lr: 2.0000e-05\n",
      "Epoch 49/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1259 - accuracy: 0.5720\n",
      "Epoch 49: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 1.1259 - accuracy: 0.5720 - val_loss: 0.9799 - val_accuracy: 0.6342 - lr: 2.0000e-05\n",
      "Epoch 50/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1204 - accuracy: 0.5807\n",
      "Epoch 50: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 37s 62ms/step - loss: 1.1204 - accuracy: 0.5807 - val_loss: 0.9797 - val_accuracy: 0.6328 - lr: 2.0000e-05\n",
      "Epoch 51/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1207 - accuracy: 0.5797\n",
      "Epoch 51: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1207 - accuracy: 0.5797 - val_loss: 0.9858 - val_accuracy: 0.6314 - lr: 2.0000e-05\n",
      "Epoch 52/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1153 - accuracy: 0.5777\n",
      "Epoch 52: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1153 - accuracy: 0.5777 - val_loss: 0.9851 - val_accuracy: 0.6317 - lr: 2.0000e-05\n",
      "Epoch 53/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1233 - accuracy: 0.5773\n",
      "Epoch 53: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1233 - accuracy: 0.5773 - val_loss: 0.9888 - val_accuracy: 0.6308 - lr: 2.0000e-05\n",
      "Epoch 54/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1109 - accuracy: 0.5835\n",
      "Epoch 54: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1109 - accuracy: 0.5835 - val_loss: 0.9789 - val_accuracy: 0.6347 - lr: 2.0000e-05\n",
      "Epoch 55/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1235 - accuracy: 0.5799\n",
      "Epoch 55: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 1.1235 - accuracy: 0.5799 - val_loss: 0.9859 - val_accuracy: 0.6336 - lr: 2.0000e-05\n",
      "Epoch 56/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1107 - accuracy: 0.5834\n",
      "Epoch 56: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1107 - accuracy: 0.5834 - val_loss: 0.9822 - val_accuracy: 0.6314 - lr: 2.0000e-05\n",
      "Epoch 57/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1155 - accuracy: 0.5787\n",
      "Epoch 57: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1155 - accuracy: 0.5787 - val_loss: 0.9815 - val_accuracy: 0.6333 - lr: 2.0000e-05\n",
      "Epoch 58/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1168 - accuracy: 0.5795\n",
      "Epoch 58: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1168 - accuracy: 0.5795 - val_loss: 0.9799 - val_accuracy: 0.6325 - lr: 2.0000e-05\n",
      "Epoch 59/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1174 - accuracy: 0.5776\n",
      "Epoch 59: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1174 - accuracy: 0.5776 - val_loss: 0.9838 - val_accuracy: 0.6308 - lr: 2.0000e-05\n",
      "Epoch 60/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1253 - accuracy: 0.5752\n",
      "Epoch 60: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1253 - accuracy: 0.5752 - val_loss: 0.9794 - val_accuracy: 0.6319 - lr: 2.0000e-05\n",
      "Epoch 61/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1182 - accuracy: 0.5809\n",
      "Epoch 61: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1182 - accuracy: 0.5809 - val_loss: 0.9824 - val_accuracy: 0.6319 - lr: 2.0000e-05\n",
      "Epoch 62/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1135 - accuracy: 0.5805\n",
      "Epoch 62: val_accuracy did not improve from 0.63555\n",
      "\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 4.000000262749381e-06.\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1135 - accuracy: 0.5805 - val_loss: 0.9907 - val_accuracy: 0.6317 - lr: 2.0000e-05\n",
      "Epoch 63/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1226 - accuracy: 0.5776\n",
      "Epoch 63: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1226 - accuracy: 0.5776 - val_loss: 0.9810 - val_accuracy: 0.6328 - lr: 4.0000e-06\n",
      "Epoch 64/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1092 - accuracy: 0.5836\n",
      "Epoch 64: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1092 - accuracy: 0.5836 - val_loss: 0.9807 - val_accuracy: 0.6328 - lr: 4.0000e-06\n",
      "Epoch 65/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1150 - accuracy: 0.5804\n",
      "Epoch 65: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1150 - accuracy: 0.5804 - val_loss: 0.9816 - val_accuracy: 0.6344 - lr: 4.0000e-06\n",
      "Epoch 66/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1082 - accuracy: 0.5855\n",
      "Epoch 66: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 63ms/step - loss: 1.1082 - accuracy: 0.5855 - val_loss: 0.9869 - val_accuracy: 0.6330 - lr: 4.0000e-06\n",
      "Epoch 67/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1026 - accuracy: 0.5852\n",
      "Epoch 67: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1026 - accuracy: 0.5852 - val_loss: 0.9819 - val_accuracy: 0.6328 - lr: 4.0000e-06\n",
      "Epoch 68/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1067 - accuracy: 0.5819\n",
      "Epoch 68: val_accuracy did not improve from 0.63555\n",
      "604/604 [==============================] - 38s 62ms/step - loss: 1.1067 - accuracy: 0.5819 - val_loss: 0.9848 - val_accuracy: 0.6317 - lr: 4.0000e-06\n",
      "Epoch 69/100\n",
      "604/604 [==============================] - ETA: 0s - loss: 1.1136 - accuracy: 0.5810\n",
      "Epoch 69: val_accuracy did not improve from 0.63555\n",
      "Restoring model weights from the end of the best epoch: 54.\n",
      "604/604 [==============================] - 39s 64ms/step - loss: 1.1136 - accuracy: 0.5810 - val_loss: 0.9803 - val_accuracy: 0.6311 - lr: 4.0000e-06\n",
      "Epoch 69: early stopping\n",
      "\n",
      "Training completed!\n",
      "Models saved in 'checkpoints/best_model.h5' and 'final_model.h5'\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:39:50.619705Z",
     "start_time": "2024-12-03T23:39:31.001634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Process a video file\n",
    "video_input_path = './videos/emotion1.mp4' \n",
    "video_output_path = './output/emotion1_processed.mp4' \n",
    "process_video(video_input_path, video_output_path)"
   ],
   "id": "41f3c40f77df52c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Opening video file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-03 16:39:31.537 Python[3897:539490] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed video saved as: ./output/emotion1_processed.mp4\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T23:41:39.168230Z",
     "start_time": "2024-12-03T23:41:34.806434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Start real-time webcam detection\n",
    "start_webcam_detection() "
   ],
   "id": "bd118f8a1b40ebb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Starting webcam...\n",
      "Webcam closed\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
